{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed819e3-c9f1-41e4-a30c-273d15ca1314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------+------+\n|    Name|date of birth|Gender|salary|\n+--------+-------------+------+------+\n|  SriRam|   1991-04-01|     M| 30000|\n| Sarthak|   2002-01-23|     M|  4000|\n|  Rohini|   1978-09-05|     M|  4000|\n|Lakshita|   2002-08-08|     F|  4000|\n|   Jenis|   1980-02-17|     F|  1200|\n+--------+-------------+------+------+\n\n+----------+-------------+------+------+\n|personname|date of birth|Gender|salary|\n+----------+-------------+------+------+\n|    SriRam|   1991-04-01|     M| 30000|\n|   Sarthak|   2002-01-23|     M|  4000|\n|    Rohini|   1978-09-05|     M|  4000|\n|  Lakshita|   2002-08-08|     F|  4000|\n|     Jenis|   1980-02-17|     F|  1200|\n+----------+-------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    " \n",
    "# Create data in dataframe\n",
    "data = [(('SriRam'), '1991-04-01', 'M', 30000),\n",
    "        (('Sarthak'), '2002-01-23', 'M', 4000),\n",
    "        (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "        (('Lakshita'), '2002-08-08', 'F', 4000),\n",
    "        (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data,\n",
    "                           schema=columns)\n",
    "df.withColumnRenamed(\"DOB\",\"date of birth\").show()\n",
    "df.withColumnRenamed(\"DOB\",\"date of birth\").withColumnRenamed(\"Name\",\"personname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81e5588-9b43-4d68-9fae-013abeeafb93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+------+\n|category|       DOB|    name|salary|\n+--------+----------+--------+------+\n|       M|1991-04-01|  SriRam| 30000|\n|       M|2002-01-23| Sarthak|  4000|\n|       M|1978-09-05|  Rohini|  4000|\n|       F|2002-08-08|Lakshita|  4000|\n|       F|1980-02-17|   Jenis|  1200|\n+--------+----------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries using select exp\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    " \n",
    "# Create data in dataframe\n",
    "data = [(('SriRam'), '1991-04-01', 'M', 30000),\n",
    "        (('Sarthak'), '2002-01-23', 'M', 4000),\n",
    "        (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "        (('Lakshita'), '2002-08-08', 'F', 4000),\n",
    "        (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data,\n",
    "                           schema=columns)\n",
    "\n",
    "data = df.selectExpr(\"Gender as category\",\"DOB\",\"Name as name\",\"salary\")\n",
    " \n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8eb2e70-4ff3-4f84-b000-3ba6c102d145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+\n|    Name|       DOB|Gender|Amount|\n+--------+----------+------+------+\n|  SriRam|1991-04-01|     M| 30000|\n| Sarthak|2002-01-23|     M|  4000|\n|  Rohini|1978-09-05|     M|  4000|\n|Lakshita|2002-08-08|     F|  4000|\n|   Jenis|1980-02-17|     F|  1200|\n+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    " \n",
    "# Select the 'salary' as 'Amount' using aliasing\n",
    "# Select remaining with their original name\n",
    "data = df.select(col(\"Name\"),col(\"DOB\"),\n",
    "                 col(\"Gender\"),\n",
    "                 col(\"salary\").alias('Amount'))\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bea0da21-8457-43ad-b1dc-d2506c28d225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Modifying Data In Pandas Dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
