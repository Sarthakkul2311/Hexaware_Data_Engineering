{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906dc656-41ae-40a6-9b56-2e16c7fb8e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 85, 76, 87, 91), ('B', 85, 76, 87, 91), ('A', 85, 78, 96, 92), ('A', 92, 76, 89, 96)]\nParallelCollectionRDD[181] at readRDDFromInputStream at PythonRDD.scala:435\nOut[14]: [('C', 85, 76, 87, 91),\n ('B', 85, 76, 87, 91),\n ('A', 85, 78, 96, 92),\n ('A', 92, 76, 89, 96)]"
     ]
    }
   ],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "print(rdd.collect())\n",
    "print(rdd) #---Transformation which gives rdd value\n",
    "\n",
    "rdd.collect() #----Action gives non rdd value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be84051a-60f9-44a4-b438-ecb3ea9b1708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\nOut[13]: [('C', 85, 76, 87, 91), ('B', 85, 76, 87, 91)]"
     ]
    }
   ],
   "source": [
    "\n",
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "print(rdd.count())\n",
    "\n",
    "rdd.take(2) ##Action gives non rdd value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a943977a-252e-4e58-96c6-8532c327fdd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\nOut[3]: 10"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "print(count_rdd.count())\n",
    "count_rdd.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adf8646-b690-4990-bb92-e83cdb12e860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\nOut[4]: 1"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "first_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "print(first_rdd.count())\n",
    "first_rdd.count()\n",
    "first_rdd.first() #First method is action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5ce17a-e311-4091-bdb3-69e5a23575ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sarthak', 'Shreya']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "filter_rdd_2 = sc.parallelize(['Rahul', 'Sarthak', 'Rohan', 'Shreya', 'Priya'])\n",
    "print(filter_rdd_2.filter(lambda x: x.startswith('S')).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517bab37-ffb7-4e97-a4cd-8cd4dfc505fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "uninon_inp = sc.parallelize([2,4,5,6,7,8,9,10])\n",
    "uninon_rdd_1 = uninon_inp.filter(lambda x:x % 2 == 0)\n",
    "uninon_rdd_2 = uninon_inp.filter(lambda x:x % 3 == 0)\n",
    "print(uninon_rdd_1.union(uninon_rdd_2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb56c62-4ef7-44f9-a190-faf9d1a8ec64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[217] at RDD at PythonRDD.scala:58\nOut[17]: PythonRDD[218] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
    "print(flatmap_rdd.flatMap(lambda x :x.split(\" \").collect()))\n",
    "flatmap_rdd.flatMap(lambda x :x.split(\" \").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0e58e8-bfe0-42a5-820d-9476464e2aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: ['Hey', 'there', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
    "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e88cd5d-42c1-45a3-8583-eb76dff7dc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "reduce_rdd = sc.parallelize([1,2,3])\n",
    "print(reduce_rdd.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c8e7b4-a8b1-4928-a05a-4899f43362e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "save_rdd = sc.parallelize([1,2,3,4,5,5])\n",
    "save_rdd.saveAsTextFile('file3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668e7898-b6ae-4643-9c8e-a04838e462b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[147] at RDD at PythonRDD.scala:58\n[11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "map_rdd = sc.parallelize([1,2,3])\n",
    "print(map_rdd.map(lambda x:x+10))\n",
    "print(map_rdd.map(lambda x:x+10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79f07c3-86cc-4b45-89e1-9c9f21639ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[150] at RDD at PythonRDD.scala:58\n[2]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "filter_rdd = sc.parallelize([1,2,3])\n",
    "print(filter_rdd.filter(lambda x:x%2==0))\n",
    "print(filter_rdd.filter(lambda x:x%2==0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c871462-fcbe-4ec2-9f42-4c0160fbede8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "print(marks_df.createOrReplaceTempView(\"dataofmarks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9bf0fc-e271-4e88-9502-37cf80565ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\nOut[10]: [1, 2, 3, 4, 5, 5]"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "print(count_rdd.take(2))\n",
    "count_rdd.take(6)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "trans & action practice",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
