{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df36962-39d5-4d3f-a860-e305ddeddb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# In loandata.csv file\n",
    "### #number of loans in each category\n",
    "### #number of people with income greater than 60000 rupees\n",
    "### #number of people with 2 or more returned cheques and income less than 50000\n",
    "### #number of people with 2 or more returned cheques and are single\n",
    "### #number of people with expenditure over 50000 a month\n",
    "### #number of members who are elgible for credit card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313cbc70-f610-4f7c-853e-0985219e5fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+-------------------+--------------+-----------+------+-----------+-------------+------------------+-----------+-------+------------+----------------+------------------+\n|Customer_ID|Age|Gender|         Occupation|Marital Status|Family Size|Income|Expenditure|Use Frequency|     Loan Category|Loan Amount|Overdue| Debt Record| Returned Cheque| Dishonour of Bill|\n+-----------+---+------+-------------------+--------------+-----------+------+-----------+-------------+------------------+-----------+-------+------------+----------------+------------------+\n|    IB14001| 30|  MALE|       BANK MANAGER|        SINGLE|          4| 50000|      22199|            6|           HOUSING| 10,00,000 |      5|      42,898|               6|                 9|\n|    IB14008| 44|  MALE|          PROFESSOR|       MARRIED|          6| 51000|      19999|            4|          SHOPPING|     50,000|      3|      33,999|               1|                 5|\n|    IB14012| 30|FEMALE|            DENTIST|        SINGLE|          3| 58450|      27675|            5|        TRAVELLING|     75,000|      6|      20,876|               3|                 1|\n|    IB14018| 29|  MALE|            TEACHER|       MARRIED|          5| 45767|      12787|            3|         GOLD LOAN|  6,00,000 |      7|      11,000|               0|                 4|\n|    IB14022| 34|  MALE|             POLICE|        SINGLE|          4| 43521|      11999|            3|        AUTOMOBILE|  2,00,000 |      2|      43,898|               1|                 2|\n|    IB14024| 55|FEMALE|              NURSE|       MARRIED|          6| 34999|      19888|            4|        AUTOMOBILE|     47,787|      1|      50,000|               0|                 3|\n|    IB14025| 39|FEMALE|            TEACHER|       MARRIED|          6| 46619|      18675|            4|           HOUSING| 12,09,867 |      8|      29,999|               6|                 8|\n|    IB14027| 51|  MALE|     SYSTEM MANAGER|       MARRIED|          3| 49999|      19111|            5|       RESTAURANTS|     60,676|      8|      13,000|               2|                 5|\n|    IB14029| 24|FEMALE|            TEACHER|        SINGLE|          3| 45008|      17454|            4|        AUTOMOBILE|  3,99,435 |      9|      51,987|               4|                 7|\n|    IB14031| 37|FEMALE|  SOFTWARE ENGINEER|       MARRIED|          5| 55999|      23999|            5|        AUTOMOBILE|     60,999|      2|           0|               5|                 3|\n|    IB14032| 24|  MALE|       DATA ANALYST|        SINGLE|          4| 60111|      28999|            6|        AUTOMOBILE|     35,232|      5|      33,333|               1|                 2|\n|    IB14034| 32|  MALE|   PRODUCT ENGINEER|       MARRIED|          6|  null|      29000|            7|COMPUTER SOFTWARES|     80,660|      6|       4,500|               5|                 4|\n|    IB14037| 54|FEMALE|            TEACHER|       MARRIED|          5| 48099|      19999|            4|       RESTAURANTS|     30,999|      1|      12,000|               7|                 5|\n|    IB14039| 45|  MALE|    ACCOUNT MANAGER|       MARRIED|          7| 45777|      18452|            4|         GOLD LOAN|  9,87,611 |      7|      39,999|               8|                 1|\n|    IB14041| 59|FEMALE|ASSISTANT PROFESSOR|       MARRIED|          4| 50999|      22999|            5|  EDUCATIONAL LOAN|  5,99,934 |      3|       9,000|               9|                 9|\n|    IB14042| 25|FEMALE|             DOCTOR|        SINGLE|          4| 60111|      27111|            5|        TRAVELLING| 12,90,929 |      4|      18,000|               1|                 0|\n|    IB14045| 31|  MALE|       STORE KEEPER|        SINGLE|          5| 40999|      11999|            3|       BOOK STORES|  1,67,654 |      1|       4,500|               0|                 1|\n|    IB14049| 49|  MALE|       BANK MANAGER|       MARRIED|          4| 45999|      14500|            4|        TRAVELLING|     79,999|      4|       6,700|               7|                 3|\n|    IB14050| 56|  MALE|     CIVIL ENGINEER|       MARRIED|          4|  null|      13999|            3|           HOUSING| 10,65,577 |      6|      19,999|               4|                 2|\n|    IB14054| 58|FEMALE|             DOCTOR|       MARRIED|          5| 60000|      25000|            5|           HOUSING|  9,00,000 |      5|      21,000|               9|                 0|\n+-----------+---+------+-------------------+--------------+-----------+------+-----------+-------------+------------------+-----------+-------+------------+----------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Loan Data\").getOrCreate()\n",
    "\n",
    "# Define the path to the file (ensure the file is uploaded to DBFS or accessible location)\n",
    "file_path = \"/FileStore/tables/loan.csv\"  # Adjust this path if needed\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "loan_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "loan_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e94fa20-6227-49e1-858b-59d6eaf4abc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n|     Loan Category|count|\n+------------------+-----+\n|           HOUSING|   67|\n|        TRAVELLING|   53|\n|       BOOK STORES|    7|\n|       AGRICULTURE|   12|\n|         GOLD LOAN|   77|\n|  EDUCATIONAL LOAN|   20|\n|        AUTOMOBILE|   60|\n|          BUSINESS|   24|\n|COMPUTER SOFTWARES|   35|\n|           DINNING|   14|\n|          SHOPPING|   35|\n|       RESTAURANTS|   41|\n|       ELECTRONICS|   14|\n|          BUILDING|    7|\n|        RESTAURANT|   20|\n|   HOME APPLIANCES|   14|\n+------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Loan Category' and count the number of loans in each category\n",
    "loan_category_counts = loan_df.groupBy(\"Loan Category\").count()\n",
    "\n",
    "# Show the result\n",
    "loan_category_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6820da81-1020-4090-995b-d1bb8a352fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with income greater than 60,000 rupees: 198\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the rows where 'Income' is greater than 60,000\n",
    "people_above_60k = loan_df.filter(col(\"Income\") > 60000)\n",
    "\n",
    "# Count the number of such people\n",
    "num_people_above_60k = people_above_60k.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of people with income greater than 60,000 rupees: {num_people_above_60k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88268ef-afcf-432a-bfb1-e466f673f363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with 2 or more returned cheques and income less than 50,000 rupees: 137\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove any leading/trailing spaces from the column names if necessary\n",
    "loan_df_cleaned = loan_df.select([col(c).alias(c.strip()) for c in loan_df.columns])\n",
    "\n",
    "# Filter the rows where 'Returned Cheque' >= 2 and 'Income' < 50,000\n",
    "people_filtered = loan_df_cleaned.filter((col(\"Returned Cheque\") >= 2) & (col(\"Income\") < 50000))\n",
    "\n",
    "# Count the number of such people\n",
    "num_people_filtered = people_filtered.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of people with 2 or more returned cheques and income less than 50,000 rupees: {num_people_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e84fed-440a-4283-8651-e1c592763db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with 2 or more returned cheques and are single: 111\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove leading/trailing spaces from column names if needed\n",
    "loan_df_cleaned = loan_df.select([col(c).alias(c.strip()) for c in loan_df.columns])\n",
    "\n",
    "# Filter the rows where 'Returned Cheque' >= 2 and 'Marital Status' is 'SINGLE'\n",
    "people_filtered = loan_df_cleaned.filter((col(\"Returned Cheque\") >= 2) & (col(\"Marital Status\") == \"SINGLE\"))\n",
    "\n",
    "# Count the number of such people\n",
    "num_people_filtered = people_filtered.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of people with 2 or more returned cheques and are single: {num_people_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef89b93-dd4f-40fd-8d92-ee7b40cc2372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with expenditure over 50,000 a month: 6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the rows where 'Expenditure' > 50,000\n",
    "people_filtered = loan_df.filter(col(\"Expenditure\") > 50000)\n",
    "\n",
    "# Count the number of such people\n",
    "num_people_filtered = people_filtered.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of people with expenditure over 50,000 a month: {num_people_filtered}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598bfb45-4066-4cb8-a3eb-c49ecf7a98fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of members eligible for a credit card: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the rows where 'Income' > 30,000 and 'Overdue' == 0 (eligible for credit card)\n",
    "eligible_members = loan_df.filter((col(\"Income\") > 30000) & (col(\"Overdue\") == 0))\n",
    "\n",
    "# Count the number of eligible members\n",
    "num_eligible_members = eligible_members.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of members eligible for a credit card: {num_eligible_members}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c85d67c7-4812-4cf0-a3d2-fdfb3f5a6b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Credit.csv file \n",
    "### #credit card users in Spain\n",
    "### #number of members who are elgible and active in the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df9b00a-9630-4b78-856e-629ae42b5054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+--------------+---------------+------+\n|RowNumber|CustomerId|  Surname|CreditScore|Geography|Gender|Age|Tenure|  Balance|NumOfProducts|IsActiveMember|EstimatedSalary|Exited|\n+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+--------------+---------------+------+\n|        1|  15634602| Hargrave|        619|   France|Female| 42|     2|        0|            1|             1|      101348.88|     1|\n|        2|  15647311|     Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|             1|      112542.58|     0|\n|        3|  15619304|     Onio|        502|   France|Female| 42|     8| 159660.8|            3|             0|      113931.57|     1|\n|        4|  15701354|     Boni|        699|   France|Female| 39|     1|        0|            2|             0|       93826.63|     0|\n|        5|  15737888| Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|             1|        79084.1|     0|\n|        6|  15574012|      Chu|        645|    Spain|  Male| 44|     8|113755.78|            2|             0|      149756.71|     1|\n|        7|  15592531| Bartlett|        822|   France|  Male| 50|     7|        0|            2|             1|        10062.8|     0|\n|        8|  15656148|   Obinna|        376|  Germany|Female| 29|     4|115046.74|            4|             0|      119346.88|     1|\n|        9|  15792365|       He|        501|   France|  Male| 44|     4|142051.07|            2|             1|        74940.5|     0|\n|       10|  15592389|       H?|        684|   France|  Male| 27|     2|134603.88|            1|             1|       71725.73|     0|\n|       11|  15767821|   Bearce|        528|   France|  Male| 31|     6|102016.72|            2|             0|       80181.12|     0|\n|       12|  15737173|  Andrews|        497|    Spain|  Male| 24|     3|        0|            2|             0|       76390.01|     0|\n|       13|  15632264|      Kay|        476|   France|Female| 34|    10|        0|            2|             0|       26260.98|     0|\n|       14|  15691483|     Chin|        549|   France|Female| 25|     5|        0|            2|             0|      190857.79|     0|\n|       15|  15600882|    Scott|        635|    Spain|Female| 35|     7|        0|            2|             1|       65951.65|     0|\n|       16|  15643966|  Goforth|        616|  Germany|  Male| 45|     3|143129.41|            2|             1|       64327.26|     0|\n|       17|  15737452|    Romeo|        653|  Germany|  Male| 58|     1|132602.88|            1|             0|        5097.67|     1|\n|       18|  15788218|Henderson|        549|    Spain|Female| 24|     9|        0|            2|             1|       14406.41|     0|\n|       19|  15661507|  Muldrow|        587|    Spain|  Male| 45|     6|        0|            1|             0|      158684.81|     0|\n|       20|  15568982|      Hao|        726|   France|Female| 24|     6|        0|            2|             1|       54724.03|     0|\n+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+--------------+---------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Load the table directly from the database into a DataFrame\n",
    "credit_df = spark.table(\"credit\")\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "credit_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25cd60e-07d1-4370-a4dc-eb89c90d2e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of credit card users in Spain: 2477\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame for users in Spain with at least one product (credit card users)\n",
    "credit_card_users_spain = credit_df.filter((col(\"Geography\") == \"Spain\") & (col(\"NumOfProducts\") > 0))\n",
    "\n",
    "# Count the number of credit card users in Spain\n",
    "num_credit_card_users_spain = credit_card_users_spain.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of credit card users in Spain: {num_credit_card_users_spain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93b516b-47f5-476e-b05f-9f07a63366ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible and active members: 5151\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame for eligible and active members\n",
    "eligible_active_members = credit_df.filter((col(\"NumOfProducts\") > 0) & (col(\"IsActiveMember\") == 1))\n",
    "\n",
    "# Count the number of eligible and active members\n",
    "num_eligible_active_members = eligible_active_members.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of eligible and active members: {num_eligible_active_members}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2369c3e-d28b-4fec-9e12-9deb6e71268c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transactions (txn.csv) file\n",
    "### #Maximum withdrawal amount in transactions\n",
    "### #MINIMUM WITHDRAWAL AMOUNT OF AN ACCOUNT in txn.csv\n",
    "### #MAXIMUM DEPOSIT AMOUNT OF AN ACCOUNT\n",
    "### #MINIMUM DEPOSIT AMOUNT OF AN ACCOUNT\n",
    "### #sum of balance in every bank account\n",
    "### #Number of transaction on each date\n",
    "### #List of customers with withdrawal amount more than 1 lakh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed006ef-2f39-407c-8a04-54277a2c440c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+----------------+-------------+-----------+\n|   Account No| TRANSACTION DETAILS|VALUE DATE| WITHDRAWAL AMT | DEPOSIT AMT |BALANCE AMT|\n+-------------+--------------------+----------+----------------+-------------+-----------+\n|409000611074'|TRF FROM  Indiafo...| 29-Jun-17|            null|      1000000|    1000000|\n|409000611074'|TRF FROM  Indiafo...|  5-Jul-17|            null|      1000000|    2000000|\n|409000611074'|FDRL/INTERNAL FUN...| 18-Jul-17|            null|       500000|    2500000|\n|409000611074'|TRF FRM  Indiafor...|  1-Aug-17|            null|      3000000|    5500000|\n|409000611074'|FDRL/INTERNAL FUN...| 16-Aug-17|            null|       500000|    6000000|\n|409000611074'|FDRL/INTERNAL FUN...| 16-Aug-17|            null|       500000|    6500000|\n|409000611074'|FDRL/INTERNAL FUN...| 16-Aug-17|            null|       500000|    7000000|\n|409000611074'|FDRL/INTERNAL FUN...| 16-Aug-17|            null|       500000|    7500000|\n|409000611074'|FDRL/INTERNAL FUN...| 16-Aug-17|            null|       500000|    8000000|\n|409000611074'|FDRL/INTERNAL FUN...| 16-Aug-17|            null|       500000|    8500000|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          133900|         null|    8366100|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|           18000|         null|    8348100|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|            5000|         null|    8343100|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          195800|         null|    8147300|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|           81600|         null|    8065700|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|           41800|         null|    8023900|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|           98500|         null|    7925400|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          143800|         null|    7781600|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          331650|         null|    7449950|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          129000|         null|    7320950|\n+-------------+--------------------+----------+----------------+-------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Load the table directly into a DataFrame\n",
    "txn_df = spark.table(\"txn\")\n",
    "\n",
    "# Show the first few rows of the DataFrame to verify the data\n",
    "txn_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac9a813-dfd5-4fec-9da1-c33f91f77fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Withdrawal Amount: 9999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Find the maximum withdrawal amount\n",
    "max_withdrawal = txn_df.agg({\" WITHDRAWAL AMT \": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Maximum Withdrawal Amount:\", max_withdrawal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f431835-7352-44aa-b55c-ed1d376473a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|   Account No|Min Withdrawal|\n+-------------+--------------+\n|     1196428'|          0.25|\n|     1196711'|          0.25|\n|409000362497'|          0.97|\n|409000405747'|       1000000|\n|409000425051'|          1.25|\n|409000438611'|           0.2|\n|409000438620'|          0.34|\n|409000493201'|       1000000|\n|409000493210'|          0.01|\n|409000611074'|         10000|\n+-------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "# Find the minimum withdrawal amount grouped by Account No\n",
    "min_withdrawal = txn_df.groupBy(\"Account No\").agg(min(\" WITHDRAWAL AMT \").alias(\"Min Withdrawal\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f434b666-649e-4df2-8d6c-514014ab0526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n|   Account No|Max Deposit|\n+-------------+-----------+\n|     1196428'|    9999999|\n|     1196711'|  999467.62|\n|409000362497'|   99977.78|\n|409000405747'|   80408.93|\n|409000425051'|       8500|\n|409000438611'|   99999.48|\n|409000438620'|     9993.8|\n|409000493201'|   94982.32|\n|409000493210'|      99.02|\n|409000611074'|     500000|\n+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "# Find the maximum deposit amount grouped by Account No\n",
    "max_deposit = txn_df.groupBy(\"Account No\").agg(max(\" DEPOSIT AMT \").alias(\"Max Deposit\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424969b4-8962-42f3-a2f6-10535526b58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n|   Account No|Min Deposit|\n+-------------+-----------+\n|     1196428'|          1|\n|     1196711'|       1.01|\n|409000362497'|       0.03|\n|409000405747'|      10000|\n|409000425051'|          1|\n|409000438611'|       0.03|\n|409000438620'|       0.07|\n|409000493201'|        0.9|\n|409000493210'|       0.01|\n|409000611074'|    1000000|\n+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "# Find the minimum deposit amount grouped by Account No\n",
    "min_deposit = txn_df.groupBy(\"Account No\").agg(min(\" DEPOSIT AMT \").alias(\"Min Deposit\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d26e0e-2c5b-4fff-b6ee-3f35d611adea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n|   Account No|       Total Balance|\n+-------------+--------------------+\n|409000438611'|-2.49486577068339...|\n|     1196711'|-1.60476498101275E13|\n|     1196428'| -8.1418498130721E13|\n|409000493210'|-3.27584952132095...|\n|409000611074'|       1.615533622E9|\n|409000425051'|-3.77211841164998...|\n|409000405747'|-2.43108047067000...|\n|409000493201'|1.0420831829499985E9|\n|409000438620'|-7.12291867951358...|\n|409000362497'| -5.2860004792808E13|\n+-------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Find the sum of balance grouped by Account No\n",
    "sum_balance = txn_df.groupBy(\"Account No\").agg(sum(\"BALANCE AMT\").alias(\"Total Balance\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8395257-75ac-45c1-a6e6-596165f9af65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|VALUE DATE|count|\n+----------+-----+\n| 23-Dec-16|  143|\n|  7-Feb-19|   98|\n| 21-Jul-15|   80|\n|  9-Sep-15|   91|\n| 17-Jan-15|   16|\n| 18-Nov-17|   53|\n| 21-Feb-18|   77|\n| 20-Mar-18|   71|\n| 19-Apr-18|   71|\n| 21-Jun-16|   97|\n| 17-Oct-17|  101|\n|  3-Jan-18|   70|\n|  8-Jun-18|  223|\n| 15-Dec-18|   62|\n|  8-Aug-16|   97|\n| 17-Dec-16|   74|\n|  3-Sep-15|   83|\n| 21-Jan-16|   76|\n|  4-May-18|   92|\n|  7-Sep-17|   94|\n+----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Find the number of transactions grouped by VALUE DATE\n",
    "transaction_count_per_date = txn_df.groupBy(\"VALUE DATE\").count().alias(\"Number of Transactions\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391424f1-a2b1-4c7a-9630-680d64998d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+----------------+\n|   Account No| TRANSACTION DETAILS|VALUE DATE| WITHDRAWAL AMT |\n+-------------+--------------------+----------+----------------+\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          133900|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          195800|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          143800|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          331650|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          129000|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          230013|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          367900|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          108000|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          141000|\n|409000611074'|INDO GIBL Indiafo...| 16-Aug-17|          206000|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          242300|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          113250|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          206900|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          276000|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          171000|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          189800|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          271323|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          200600|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          176900|\n|409000611074'|INDO GIBL Indiafo...|  6-Sep-17|          150050|\n+-------------+--------------------+----------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the data for withdrawal amount greater than 1 lakh\n",
    "customers_with_high_withdrawal = txn_df.filter(col(\" WITHDRAWAL AMT \") > 100000).select(\"Account No\", \"TRANSACTION DETAILS\", \"VALUE DATE\", \" WITHDRAWAL AMT \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba335a34-a657-403a-87e9-805e08b889ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CaseStudy(PySpark)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
